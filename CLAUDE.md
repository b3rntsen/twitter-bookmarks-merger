# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Twitter/X bookmarks merger tool that processes exports from the "X Bookmarks Exporter" Chrome extension. It deduplicates bookmarks across multiple exports, consolidates downloaded media, generates browsable HTML pages, and uses Claude API for AI-powered categorization.

## Commands

```bash
# Activate virtual environment first
source .venv/bin/activate

# Processing commands
python3 tools/bookmark_merger.py merge        # Dedupe JSON → master/bookmarks.json
python3 tools/bookmark_merger.py consolidate  # Consolidate media → master/media/
python3 tools/bookmark_merger.py categorize   # AI categorization (needs ANTHROPIC_API_KEY)
python3 tools/bookmark_merger.py generate     # Generate HTML → master/html/
python3 tools/bookmark_merger.py export       # Export for NotebookLM → master/exports/
python3 tools/bookmark_merger.py all          # Run merge, consolidate, generate, export

# Incremental update (preferred for adding new exports)
python3 tools/bookmark_merger.py update       # Merge new, categorize NEW only, regenerate

# AI Stories (generates narrative timelines per category/year)
python3 tools/bookmark_merger.py stories                  # Generate all stories
python3 tools/bookmark_merger.py stories --list-categories # Show available categories
python3 tools/bookmark_merger.py stories --category ai-machine-learning  # Specific category
python3 tools/bookmark_merger.py stories --force-year 2025 # Force regenerate year

# AI Authors (categorizes authors by profile type)
python3 tools/bookmark_merger.py authors                  # Categorize + generate HTML
python3 tools/bookmark_merger.py authors categorize       # AI categorization only
python3 tools/bookmark_merger.py authors generate         # Generate HTML only
python3 tools/bookmark_merger.py authors --min-bookmarks 5 # Only categorize authors with 5+ bookmarks

# Publishing to GitHub Pages (uses Twitter CDN for media, no local files needed)
python3 tools/bookmark_merger.py publish    # Publish to dethele.com/twitter
python3 tools/bookmark_merger.py unpublish  # Remove from dethele.com (DESTRUCTIVE)

# Server deployment (to twitter.dethele.com with local media)
python3 tools/bookmark_merger.py publish-server  # Generate HTML → server/html/
./scripts/deploy-bookmarks.sh                     # Sync HTML + media to EC2
./scripts/deploy-bookmarks.sh --html-only         # Sync only HTML (faster)

# Cleanup commands
python3 tools/bookmark_merger.py clean        # Delete master/ to re-run (safe)
python3 tools/bookmark_merger.py cleanup-raw  # Delete raw/ (DESTRUCTIVE, separate step)
```

## Architecture

**Directory Structure:**
- `raw/json/` - Original JSON exports from XBookmarksExporter
- `raw/media/` - Original media export folders (named by export date)
- `master/` - Generated output (can be safely deleted and regenerated)
- `server/` - Server-optimized HTML (generated by `publish-server`)
- `tools/` - CLI processing tools (bookmark_merger.py)
- `web/` - Django app ("new-gen" - automatic Twitter scraping, not yet working)

**Data Flow:**
1. Raw JSON exports deduplicated by Tweet ID (keeps most recent `Scraped At`)
2. Media files from all exports merged (skips `.crdownload` incomplete files)
3. AI categorization discovers taxonomy, assigns categories, generates summaries
4. HTML generation creates navigable pages with embedded local media

**Configuration:**
- `.env` - API keys, OAuth credentials, database settings (see `.env.example`)
- Script auto-loads `.env` at startup

## Server Deployment

The site can be deployed to `twitter.dethele.com` with Google OAuth protection:

**URLs:**
- `/` - Static bookmarks HTML (from `tools/`)
- `/new-gen/` - Django app for automatic Twitter scraping (from `web/`)
- `/accounts/` - OAuth login via Google

**Deployment:**
```bash
# Generate server HTML (uses absolute /media/bookmarks/ paths)
python3 tools/bookmark_merger.py publish-server

# Deploy to EC2 (requires SSH access)
./scripts/deploy-bookmarks.sh          # Full deploy (HTML + 12GB media)
./scripts/deploy-bookmarks.sh --html-only  # Quick deploy (HTML only)
```

**Docker (on server):**
```bash
docker-compose -f docker-compose.prod.yml up -d --build
```

## Important Rules

- **Adding new exports**: Use `update` command - it only categorizes NEW bookmarks, preserving existing categorizations and saving API calls.
- **`clean` vs `cleanup-raw`**: These are intentionally separate commands. `clean` removes generated files and is safe to run anytime. `cleanup-raw` deletes original export data and should only be run after QA.
- **Re-running**: To regenerate all output, run `clean` then `all`. Never need to delete raw data to re-process.
- **Dependencies**: When adding new Python dependencies, add them to `requirements.txt`.
